## Amazon S3 (Simple Storage Service)

AWS S3 is a serverless service that provides object-level storage. S3 **stores data as objects in buckets**. A bucket is combination of a bucket name, key, and version ID. It is used for **data that doesn't change often** i.e. ideal for storing static web content, media, backups, and archiving.

Use cases include storing log files, media, audio, video, images, CI/CD artifacts, static websites, etc. Traditionally, it is very expensive to store files on the web server and it would not be scalable.

### Features

- Offers unlimited storage space
- you can upload any type of file
- Maximum file size of a single file is 5TB; when uploading more than 5GB, you must use **multi-part upload**
- Each bucket name must be unique across all AWS accounts in all AWS Regions within a partition
- A partition is a grouping of Regions
- Buckets are defined at the **Region** level i.e. supports global namespace but the buckets are regional
- An AWS account supports 100 buckets by default; you can increase to 1,000 by requesting a service limit increase
- Has data encryption automatically enabled

### Accessing

- AWS console
- AWS CLI
- SDK
- REST API

### Versioning

When you upload a file, you can set permissions to control visibility and access to it. You can also use the **versioning feature to track changes to your objects** over time (at bucket level).

For objects without versions, AWS will create a version marker (delete marker) if the object gets deleted. You can restore your object by deleting the delete marker. If you delete an object with a specific version, that object will get **permanently deleted**.

Once you enable versioning, you cannot disable it. The only option is you can suspend versioning (version ID of NULL). It does not delete existing versions.

### Prefixing

S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.

There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes: if you have a file f1 stored in an S3 object path like so s3://your_bucket_name/folder1/sub_folder_1/f1, then /folder1/sub_folder_1/ becomes the prefix for file f1.

### Object model

Amazon S3 model is a **flat structure i.e. no hierarchy of sub-buckets or subfolders**. However, it supports the concept of folders by using key name prefixes and delimiters.

```
http://testbucket.s3.amazonaws.com/2022-03-01/AmazonS3.html
http://testbucket.s3.amazonaws.com/2022-03-01/Cats.jpg
```

### Object Lock

S3 Object Lock prevents object deletion or modification for a fixed period, helping to comply with regulatory requirements.

Object Lock has two modes:

- Governance mode: Only users with special permissions can modify settings
- Compliance mode: No user (including root) can modify settings

## Server-side Encryption (SSE)

### SSE-C (client-managed keys)

You provide the key temporarily by submitting the encryption details that states you have the encryption keys in order to manage S3 objects. You provide the encryption key when submitting requests to S3.

### SSE-SE (S3 managed keys)

Default for S3.

### SSE-KMS

- AWS-managed CMK
- Customer-managed CMK

## Permissions

Permissions are evaluated in the following order:

1. Public access firewall
2. IAM policies
3. Bucket policies
4. ACLs

### Bucket policies

Unlike IAM policies which are attached to resources and users in AWS, S3 bucket policies can only be attached to S3 buckets. The policy that is placed on the bucket applies to every object in that bucket. It can also be used to deny uploads that are not encrypted.

Bucket policies are useful for **cross-account access** i.e. other AWS accounts. Also, bucket policies are required to allow public access i.e. users who are not from AWS.

An IAM principal can access an S3 object if:

- The user IAM permissions ALLOW it OR the bucket policy ALLOWS it
- AND there is no explicit DENY

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowJohnDoe", // Optional identifier for your reference.
      "Principal": {
        "AWS": ["arn:aws:iam::1111222333:user/JohnDoe"]
      },
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3::DOC-EXAMPLE-BUCKET/*"]
    }
  ]
}
```

### Access Control Lists (ACLs)

S3 ACLs can grant read and write permissions to a single user on specific S3 objects, allowing for fine-grained access control.

### Access points

When you have multiple users accessing a bucket, you may end up with a complex policy with different levels of access to the varying objects. Instead of users pointing to the bucket, they will point to the unique ARN. Every group/user can be given their own access point which acts as their own view/tunnel into S3 bucket.

Instead of applying policies on buckets, we can move the policies down to the access points, which makes the policies more manageable. Also, you can restrict access to bucket to devices in VPCs.

## Replication

To use replication, you need to **enable versioning in source and destination buckets**. Two kinds of replication:

- Cross-Region Replication (CRR)
- Same-Region Replication (SRR)

Copying is done asynchronously.

## Storage lifecycle

If you keep manually changing your objects from storage tier to storage tier, you can automate the process by configuring their Amazon S3 lifecycle. You can choose to automate between two types of actions:

- **Transition**: Define when objects should transition to another storage class
- **Expiration**: Define when objects expire and should be permanently deleted

## Storage classes

With AWS S3, you pay for what you use. You can choose a range of storage classes to select a fit for your business and cost needs. When selecting a storage class, consider the following:

- How often you plan to retrieve your data
- How available you need your data to be

### Standard

- Designed for frequently used accessed data
- Stores data in a **minimum of three Availability Zones**
- You are charged for ingress data per GB i.e. retrieving data from S3.

Provides high availability for objects. Makes a good choice for a wide range of use cases, including websites, content distribution, and data analytics.

### Standard-Infrequent Access (S3 Standard-IA)

- Ideal for infrequently accessed data
- Has a lower storage price than Standard
- Has a retrieval fee with minimum duration charge of 30 days
- Minimum size charge of 128KB per object

### One Zone-Infrequent Access (S3 One Zone-IA)

Stores data in a single Availability Zone. Has a lower storage price than S3 Standard-IA. This makes a good storage class if:

- You want to save costs on storage
- Data is still replicated within the same AZ
- You can easily reproduce your data in the event of an Availability Zone failure
- You want to store secondary backup copies

### Intelligent-Tiering

- Ideal for data with unknown or changing access patterns
- Requires a small monthly monitoring and automation fee per object
- No retrieval charges

In this storage class, S3 monitors objects' access patterns:

- Frequent access tier
- Infrequent access tier if objects not accessed for 30 days
- Archive Instant access tier if objects not accessed for 90 days

### Glacier Instant Retrieval

- Works well for archived data that requires immediate access
- Can retrieve objects within a few miliseconds
- Minimum duration charge of 90 days
- Higher retrieval cost

### Glacier Flexible Retrieval

- Low-cost storage designed for data archiving
- Does not provide instant retrieval
- Options for retrieval include Standard (3-5h), Bulk (5-12h), and Expedited (1-5 minutes)
- Minimum size charge of 40KB per object
- During retrieval, objects are stored in S3 Standard-IA class temporarily

### Glacier Deep Archive

- Lowest-cost object storage class ideal for archiving
- Able to retrieve objects within 12 hours
- Minimum duration charge of 180 days

## S3 Transfer Acceleration

You can increase the transfer speed by transferring a file to an AWS edge location which will forward the data to the S3 bucket in the targeted region using AWS **internal network**.

```
File in USA -> Edge Location (USA) -> S3 Bucket (Australia)
```

## Static website hosting

- Additional charge per request sent to the static site
- Gives a URL to access your website e.g. `http://bucketname.s3-website-<region-name>.amazonaws.com`

## Pre-signed URLs

If you need to share files on S3 with a public user, you can use pre-signed URLs which has your authentication details baked into it. The public user will have the same privileges as the AWS user.
